{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_UM7V4lluh6"
      },
      "source": [
        "# BERT4Rec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxdMxhAKluiJ"
      },
      "source": [
        "## Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QcTX4Wz3luiK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ksh/miniconda3/envs/modeling/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import mlflow\n",
        "from mlflow.models.signature import ModelSignature\n",
        "from mlflow.types.schema import Schema, ColSpec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJeNNSfbluiK"
      },
      "source": [
        "## Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pthtFJCXluiL"
      },
      "outputs": [],
      "source": [
        "# model setting\n",
        "max_len = 20\n",
        "hidden_units = 50\n",
        "num_heads = 1\n",
        "num_layers = 2\n",
        "dropout_rate=0.5\n",
        "num_workers = 1\n",
        "device = 'cuda'\n",
        "\n",
        "# training setting\n",
        "lr = 0.001\n",
        "batch_size = 128\n",
        "num_epochs = 200\n",
        "mask_prob = 0.15 # for cloze task\n",
        "\n",
        "params = {\n",
        "    \"max_len\":20,\n",
        "    \"hidden_units\":50,\n",
        "    \"num_heads\":1,\n",
        "    \"num_layers\":2,\n",
        "    \"dropout_rate\":0.5,\n",
        "    \"device\":'cuda',\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03ZrXEWnluiL"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "Data preprocessing은 SASRec과 달리 cloze task 수행을 위한 masking을 추가해주어야 합니다. 그 외의 부분은 SASRec과 유사합니다. Masking이 추가 되었다는 것에 주의하여 Data preprocessing을 구현해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m FOLDER_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m FILE_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m FILE_PATH \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(FOLDER_PATH, FILE_NAME)\n\u001b[1;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(FILE_PATH)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "############# 중요 #############\n",
        "# data_path는 사용자의 디렉토리에 맞게 설정해야 합니다.\n",
        "FOLDER_PATH = \"./data\"\n",
        "\n",
        "FILE_NAME = \"bert.csv\"\n",
        "FILE_PATH = os.path.join(FOLDER_PATH, FILE_NAME)\n",
        "\n",
        "df = pd.read_csv(FILE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "item_ids = df['item'].unique()\n",
        "user_ids = df['user'].unique()\n",
        "num_item, num_user = len(item_ids), len(user_ids)\n",
        "num_batch = num_user // batch_size\n",
        "\n",
        "# user, item indexing\n",
        "item2idx = pd.Series(data=np.arange(len(item_ids))+1, index=item_ids) # item re-indexing (1~num_item), num_item+1: mask idx\n",
        "user2idx = pd.Series(data=np.arange(len(user_ids)), index=user_ids) # user re-indexing (0~num_user-1)\n",
        "\n",
        "# dataframe indexing\n",
        "df = pd.merge(df, pd.DataFrame({'item': item_ids, 'item_idx': item2idx[item_ids].values}), on='item', how='inner')\n",
        "df = pd.merge(df, pd.DataFrame({'user': user_ids, 'user_idx': user2idx[user_ids].values}), on='user', how='inner')\n",
        "df.sort_values(['user_idx', 'time'], inplace=True)\n",
        "del df['item'], df['user']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8587"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([   0,    1,    2, ..., 2575, 2576, 2577])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"user_idx\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1fkE92IkluiL",
        "outputId": "60e36360-6f61-4ffc-9ebd-c4682d0847e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num users: 2578, num items: 8587\n"
          ]
        }
      ],
      "source": [
        "# train set, valid set 생성\n",
        "users = defaultdict(list) # defaultdict은 dictionary의 key가 없을때 default 값을 value로 반환\n",
        "user_train = {}\n",
        "user_valid = {}\n",
        "for u, i, t in zip(df['user_idx'], df['item_idx'], df['time']):\n",
        "    users[u].append(i)\n",
        "\n",
        "for user in users:\n",
        "    user_train[user] = users[user][:-1]\n",
        "    user_valid[user] = [users[user][-1]]\n",
        "\n",
        "print(f'num users: {num_user}, num items: {num_item}')\n",
        "\n",
        "params[\"num_item\"] = num_item"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubmuAnX-luiM"
      },
      "source": [
        "SASRec에서는 sample function을 따로 구현했지만, BERT4Rec에서는 pytorch의 내장 data loader를 사용하여서 구현해봅시다. Custom dataset을 구현하기 위해서는 pytorch의 dataset class를 상속받아 생성한 class의 구현이 필요합니다. 자세한 문법은 https://pytorch.org/tutorials/beginner/basics/data_tutorial.html 에서 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7qB9OEnkluiM"
      },
      "outputs": [],
      "source": [
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, user_train, num_user, num_item, max_len, mask_prob):\n",
        "        self.user_train = user_train\n",
        "        self.num_user = num_user\n",
        "        self.num_item = num_item\n",
        "        self.max_len = max_len\n",
        "        self.mask_prob = mask_prob\n",
        "\n",
        "    def __len__(self):\n",
        "        # 총 user의 수 = 학습에 사용할 sequence의 수\n",
        "        return self.num_user\n",
        "\n",
        "    def __getitem__(self, user):\n",
        "        # iterator를 구동할 때 사용됩니다.\n",
        "        seq = self.user_train[user]\n",
        "        tokens = []\n",
        "        labels = []\n",
        "        for s in seq:\n",
        "            prob = np.random.random() # TODO1: numpy를 사용해서 0~1 사이의 임의의 값을 샘플링하세요.\n",
        "            if prob < self.mask_prob:\n",
        "                prob /= self.mask_prob\n",
        "\n",
        "                # BERT 학습\n",
        "                if prob < 0.8:\n",
        "                    # masking\n",
        "                    tokens.append(self.num_item + 1)  # mask_index: num_item + 1, 0: pad, 1~num_item: item index\n",
        "                elif prob < 0.9:\n",
        "                    tokens.append(np.random.randint(1, self.num_item+1))  # item random sampling\n",
        "                else:\n",
        "                    tokens.append(s)\n",
        "                labels.append(s)  # 학습에 사용\n",
        "            else:\n",
        "                tokens.append(s)\n",
        "                labels.append(0)  # 학습에 사용 X, trivial\n",
        "        tokens = tokens[-self.max_len:]\n",
        "        labels = labels[-self.max_len:]\n",
        "        mask_len = self.max_len - len(tokens)\n",
        "\n",
        "        # zero padding\n",
        "        tokens = [0] * mask_len + tokens\n",
        "        labels = [0] * mask_len + labels\n",
        "        return torch.LongTensor(tokens), torch.LongTensor(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSmSpRNSluiM"
      },
      "source": [
        "## Model\n",
        "\n",
        "Multi-head attention 부분은 SASRec과 같습니다. 다만 point-wise feed forward network에서 GeLU activation을 사용한다는 차이점이 있습니다. 그리고 point-wise feed forward network의 MLP의 dimension을 SASRec에서는 일정하게 유지시켰지만 BERT4Rec에서는 NLP에서 사용하는 모델과 같이 dimension에 4배 차이를 두었습니다. Positional encoding 역시 SASRec과 마찬가지로 학습 가능하도록 모델링합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2Z3GYx1jluiN"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
        "\n",
        "    def forward(self, Q, K, V, mask):\n",
        "        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units)\n",
        "        attn_score = attn_score.masked_fill(mask == 0, -1e9)  # 유사도가 0인 지점은 -infinity로 보내 softmax 결과가 0이 되도록 함\n",
        "        attn_dist = self.dropout(F.softmax(attn_score, dim=-1))  # attention distribution\n",
        "        output = torch.matmul(attn_dist, V)  # dim of output : batchSize x num_head x seqLen x hidden_units\n",
        "        return output, attn_dist\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads # head의 수\n",
        "        self.hidden_units = hidden_units\n",
        "        self.hidden_units_per_head = self.hidden_units // self.num_heads\n",
        "\n",
        "        # query, key, value, output 생성을 위해 Linear 모델 생성\n",
        "        self.W_Q = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "        self.W_K = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "        self.W_V = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "        self.W_O = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(hidden_units, dropout_rate) # scaled dot product attention module을 사용하여 attention 계산\n",
        "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
        "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
        "\n",
        "    def forward(self, enc, mask):\n",
        "        residual = enc # residual connection을 위해 residual 부분을 저장\n",
        "        batch_size, seqlen = enc.size(0), enc.size(1)\n",
        "\n",
        "        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n",
        "        \n",
        "        # Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units)\n",
        "        # K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units)\n",
        "        # V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units)\n",
        "        \n",
        "        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units_per_head)\n",
        "        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units_per_head)\n",
        "        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units_per_head)\n",
        "\n",
        "        # Head별로 각기 다른 attention이 가능하도록 Transpose 후 각각 attention에 통과시킴\n",
        "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2)\n",
        "        output, attn_dist = self.attention(Q, K, V, mask)\n",
        "\n",
        "        # 다시 Transpose한 후 모든 head들의 attention 결과를 합칩니다.\n",
        "        output = output.transpose(1, 2).contiguous()\n",
        "        output = output.view(batch_size, seqlen, -1)\n",
        "\n",
        "        # Linear Projection, Dropout, Residual sum, and Layer Normalization\n",
        "        output = self.layerNorm(self.dropout(self.W_O(output)) + residual)\n",
        "        return output, attn_dist\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "\n",
        "        # SASRec과의 dimension 차이가 있습니다.\n",
        "        self.W_1 = nn.Linear(hidden_units, 4 * hidden_units)\n",
        "        self.W_2 = nn.Linear(4 * hidden_units, hidden_units)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        output = self.W_2(F.gelu(self.dropout(self.W_1(x)))) # activation: relu -> gelu\n",
        "        output = self.layerNorm(self.dropout(output) + residual)\n",
        "        return output\n",
        "\n",
        "class BERT4RecBlock(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
        "        super(BERT4RecBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(num_heads, hidden_units, dropout_rate)\n",
        "        self.pointwise_feedforward = PositionwiseFeedForward(hidden_units, dropout_rate)\n",
        "\n",
        "    def forward(self, input_enc, mask):\n",
        "        output_enc, attn_dist = self.attention(input_enc, mask)\n",
        "        output_enc = self.pointwise_feedforward(output_enc)\n",
        "        return output_enc, attn_dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZYVhTP9luiN"
      },
      "source": [
        "### BERT4Rec\n",
        "\n",
        "위에서 구현한 class를 가지고 BERT4Rec을 구현해봅시다. BERT4Rec의 item embedding은 item의 개수에 비해 2개 더 많게 설정해야 합니다. 이는 padding과 cloze task를 위한 mask를 표기하기 위함입니다. 최종적으로 다음 item을 예측할 때도 전체 log에 mask index를 추가하여 예측을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5XjnOBeOluiN"
      },
      "outputs": [],
      "source": [
        "# model setting\n",
        "class BERT4Rec(nn.Module):\n",
        "    def __init__(self, **params):\n",
        "        super(BERT4Rec, self).__init__()\n",
        "        \n",
        "        self.num_item = params[\"num_item\"]\n",
        "        self.hidden_units = params[\"hidden_units\"]\n",
        "        self.num_heads = params[\"num_heads\"]\n",
        "        self.num_layers = params[\"num_layers\"]\n",
        "        self.max_len = params[\"max_len\"]\n",
        "        self.dropout_rate = params[\"dropout_rate\"]\n",
        "        self.device = params[\"device\"]\n",
        "\n",
        "        self.item_emb = nn.Embedding(self.num_item + 2, self.hidden_units, padding_idx=0) # TODO2: mask와 padding을 고려하여 embedding을 생성해보세요.\n",
        "        self.pos_emb = nn.Embedding(self.max_len, self.hidden_units) # learnable positional encoding\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\n",
        "        self.emb_layernorm = nn.LayerNorm(self.hidden_units, eps=1e-6)\n",
        "\n",
        "        self.blocks = nn.ModuleList([BERT4RecBlock(self.num_heads, self.hidden_units, self.dropout_rate) for _ in range(self.num_layers)])\n",
        "        self.out = nn.Linear(self.hidden_units, self.num_item + 1) # TODO3: 예측을 위한 output layer를 구현해보세요. (num_item 주의)\n",
        "\n",
        "    def forward(self, log_seqs):\n",
        "        seqs = self.item_emb(torch.LongTensor(log_seqs).to(self.device))\n",
        "        positions = np.tile(np.array(range(log_seqs.shape[1])), [log_seqs.shape[0], 1])\n",
        "        seqs += self.pos_emb(torch.LongTensor(positions).to(self.device))\n",
        "        seqs = self.emb_layernorm(self.dropout(seqs))\n",
        "\n",
        "        mask = torch.BoolTensor(log_seqs > 0).unsqueeze(1).repeat(1, log_seqs.shape[1], 1).unsqueeze(1).to(self.device) # mask for zero pad\n",
        "        for block in self.blocks:\n",
        "            seqs, attn_dist = block(seqs, mask)\n",
        "        out = self.out(seqs)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ht5ZjLxluiN"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'max_len': 20, 'hidden_units': 50, 'num_heads': 1, 'num_layers': 2, 'dropout_rate': 0.5, 'device': 'cuda', 'num_item': 8587}\n"
          ]
        }
      ],
      "source": [
        "print(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NFrvnq9PluiN"
      },
      "outputs": [],
      "source": [
        "model = BERT4Rec(**params)\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0) # label이 0인 경우 무시\n",
        "seq_dataset = SeqDataset(user_train, num_user, num_item, max_len, mask_prob)\n",
        "data_loader = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, pin_memory=True) # TODO4: pytorch의 DataLoader와 seq_dataset을 사용하여 학습 파이프라인을 구현해보세요.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recall_and_precision_at_k(user_true_items, user_predicted_items, k=10):\n",
        "    num_hits = 0\n",
        "    num_true_items = 0\n",
        "    num_predicted_items = 0\n",
        "\n",
        "    for user in user_true_items.keys():\n",
        "        true_items = set(user_true_items[user])\n",
        "        predicted_items = set(user_predicted_items[user][:k])\n",
        "\n",
        "        num_hits += len(true_items & predicted_items)\n",
        "        num_true_items += len(true_items)\n",
        "        num_predicted_items += len(predicted_items)\n",
        "\n",
        "    recall = num_hits / float(num_true_items) if num_true_items > 0 else 0\n",
        "    precision = num_hits / float(num_predicted_items) if num_predicted_items > 0 else 0\n",
        "    return recall, precision\n",
        "\n",
        "def f1_score(recall, precision):\n",
        "    return 2 * (precision * recall) / (precision + recall+1e-8) \n",
        "\n",
        "def evaluation():\n",
        "    def random_neg(l, r, s):\n",
        "        t = np.random.randint(l, r)\n",
        "        while t in s:\n",
        "            t = np.random.randint(l, r)\n",
        "        return t\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    NDCG = 0.0\n",
        "    HIT = 0.0\n",
        "    total_recall = 0.0\n",
        "    total_precision = 0.0\n",
        "\n",
        "    num_user_sample = num_user\n",
        "    users = range(0, num_user)\n",
        "    k= 10\n",
        "    for u in users:\n",
        "        seq = (user_train[u])[-max_len:] \n",
        "        rated = set(user_train[u] + user_valid[u])\n",
        "        neg = list((set(item_ids)-set(rated)))\n",
        "        # 아이템 하나 당 negative sampling의 개수 지정\n",
        "        item_idx = [user_valid[u][0]] + [random_neg(0, len(item_ids), rated) for _ in range(45)]\n",
        "        \n",
        "        seq_tensor = torch.tensor(seq, dtype=torch.long).unsqueeze(0)\n",
        "        item_idx_tensor = torch.tensor(item_idx, dtype=torch.long)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = -model(seq_tensor)\n",
        "            predictions = predictions[0, -1, item_idx_tensor]\n",
        "            rank = predictions.argsort().argsort()[0].item()\n",
        "\n",
        "        if rank < k:\n",
        "            NDCG += 1 / np.log2(rank + 2)\n",
        "            HIT += 1\n",
        "            total_recall += 1 / min(len(user_valid[u]), k)\n",
        "            total_precision += 1 / k\n",
        "\n",
        "    recall = total_recall / num_user_sample\n",
        "    precision = total_precision / num_user_sample\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "\n",
        "    print(f'NDCG@10: {NDCG/num_user_sample}')\n",
        "    print(f'Recall@10: {recall}')\n",
        "    print(f'Precision@10: {precision}')\n",
        "    print(f'F1: {f1_score}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mlflow.set_tracking_uri(uri=\"http://101.79.11.75:8010\")\n",
        "# mlflow.set_experiment(\"bert4rec\")\n",
        "# mlflow.pytorch.autolog()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CanKdLVvluiO",
        "outputId": "5bc6e520-7b19-40f0-d970-c73d548319f3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   1| Step:  20| Train loss: 9.07810: 100%|██████████| 21/21 [00:00<00:00, 24.66it/s]\n",
            "Epoch:   2| Step:  20| Train loss: 8.42594: 100%|██████████| 21/21 [00:00<00:00, 72.21it/s]\n",
            "Epoch:   3| Step:  20| Train loss: 7.85969: 100%|██████████| 21/21 [00:00<00:00, 75.64it/s]\n",
            "Epoch:   4| Step:  20| Train loss: 7.91114: 100%|██████████| 21/21 [00:00<00:00, 75.96it/s]\n",
            "Epoch:   5| Step:  20| Train loss: 6.96334: 100%|██████████| 21/21 [00:00<00:00, 75.65it/s]\n",
            "Epoch:   6| Step:  20| Train loss: 5.50122: 100%|██████████| 21/21 [00:00<00:00, 74.46it/s]\n",
            "Epoch:   7| Step:  20| Train loss: 8.44214: 100%|██████████| 21/21 [00:00<00:00, 75.92it/s]\n",
            "Epoch:   8| Step:  20| Train loss: 7.94709: 100%|██████████| 21/21 [00:00<00:00, 75.77it/s]\n",
            "Epoch:   9| Step:  20| Train loss: 6.92905: 100%|██████████| 21/21 [00:00<00:00, 72.15it/s]\n",
            "Epoch:  10| Step:  20| Train loss: 8.82403: 100%|██████████| 21/21 [00:00<00:00, 75.94it/s]\n",
            "Epoch:  11| Step:  20| Train loss: 8.81872: 100%|██████████| 21/21 [00:00<00:00, 76.01it/s]\n",
            "Epoch:  12| Step:  20| Train loss: 7.87465: 100%|██████████| 21/21 [00:00<00:00, 71.35it/s]\n",
            "Epoch:  13| Step:  20| Train loss: 7.68096: 100%|██████████| 21/21 [00:00<00:00, 71.50it/s]\n",
            "Epoch:  14| Step:  20| Train loss: 5.50768: 100%|██████████| 21/21 [00:00<00:00, 75.67it/s]\n",
            "Epoch:  15| Step:  20| Train loss: 6.66620: 100%|██████████| 21/21 [00:00<00:00, 74.95it/s]\n",
            "Epoch:  16| Step:  20| Train loss: 7.34914: 100%|██████████| 21/21 [00:00<00:00, 71.16it/s]\n",
            "Epoch:  17| Step:  20| Train loss: 5.10385: 100%|██████████| 21/21 [00:00<00:00, 75.84it/s]\n",
            "Epoch:  18| Step:  20| Train loss: 8.39330: 100%|██████████| 21/21 [00:00<00:00, 74.36it/s]\n",
            "Epoch:  19| Step:  20| Train loss: 8.09839: 100%|██████████| 21/21 [00:00<00:00, 75.30it/s]\n",
            "Epoch:  20| Step:  20| Train loss: 7.71235: 100%|██████████| 21/21 [00:00<00:00, 68.93it/s]\n",
            "Epoch:  21| Step:  20| Train loss: 5.78626: 100%|██████████| 21/21 [00:00<00:00, 72.48it/s]\n",
            "Epoch:  22| Step:  20| Train loss: 8.69431: 100%|██████████| 21/21 [00:00<00:00, 68.94it/s]\n",
            "Epoch:  23| Step:  20| Train loss: 8.02049: 100%|██████████| 21/21 [00:00<00:00, 70.56it/s]\n",
            "Epoch:  24| Step:  20| Train loss: 5.96144: 100%|██████████| 21/21 [00:00<00:00, 71.74it/s]\n",
            "Epoch:  25| Step:  20| Train loss: 8.39391: 100%|██████████| 21/21 [00:00<00:00, 66.54it/s]\n",
            "Epoch:  26| Step:  20| Train loss: 6.55067: 100%|██████████| 21/21 [00:00<00:00, 68.35it/s]\n",
            "Epoch:  27| Step:  20| Train loss: 8.55349: 100%|██████████| 21/21 [00:00<00:00, 69.44it/s]\n",
            "Epoch:  28| Step:  20| Train loss: 7.47449: 100%|██████████| 21/21 [00:00<00:00, 75.74it/s]\n",
            "Epoch:  29| Step:  20| Train loss: 8.00869: 100%|██████████| 21/21 [00:00<00:00, 75.47it/s]\n",
            "Epoch:  30| Step:  20| Train loss: 7.13805: 100%|██████████| 21/21 [00:00<00:00, 71.09it/s]\n",
            "Epoch:  31| Step:  20| Train loss: 8.29911: 100%|██████████| 21/21 [00:00<00:00, 75.93it/s]\n",
            "Epoch:  32| Step:  20| Train loss: 7.02941: 100%|██████████| 21/21 [00:00<00:00, 71.78it/s]\n",
            "Epoch:  33| Step:  20| Train loss: 7.84989: 100%|██████████| 21/21 [00:00<00:00, 75.92it/s]\n",
            "Epoch:  34| Step:  20| Train loss: 8.11882: 100%|██████████| 21/21 [00:00<00:00, 69.15it/s]\n",
            "Epoch:  35| Step:  20| Train loss: 5.91876: 100%|██████████| 21/21 [00:00<00:00, 59.81it/s]\n",
            "Epoch:  36| Step:  20| Train loss: 7.01840: 100%|██████████| 21/21 [00:00<00:00, 69.43it/s]\n",
            "Epoch:  37| Step:  20| Train loss: 7.55117: 100%|██████████| 21/21 [00:00<00:00, 70.53it/s]\n",
            "Epoch:  38| Step:  20| Train loss: 6.14847: 100%|██████████| 21/21 [00:00<00:00, 59.28it/s]\n",
            "Epoch:  39| Step:  20| Train loss: 7.15692: 100%|██████████| 21/21 [00:00<00:00, 70.69it/s]\n",
            "Epoch:  40| Step:  20| Train loss: 4.00753: 100%|██████████| 21/21 [00:00<00:00, 67.22it/s]\n",
            "Epoch:  41| Step:  20| Train loss: 8.21225: 100%|██████████| 21/21 [00:00<00:00, 73.99it/s]\n",
            "Epoch:  42| Step:  20| Train loss: 7.93275: 100%|██████████| 21/21 [00:00<00:00, 74.31it/s]\n",
            "Epoch:  43| Step:  20| Train loss: 6.65393: 100%|██████████| 21/21 [00:00<00:00, 75.53it/s]\n",
            "Epoch:  44| Step:  20| Train loss: 5.80828: 100%|██████████| 21/21 [00:00<00:00, 68.78it/s]\n",
            "Epoch:  45| Step:  20| Train loss: 3.59708: 100%|██████████| 21/21 [00:00<00:00, 72.08it/s]\n",
            "Epoch:  46| Step:  20| Train loss: 7.40225: 100%|██████████| 21/21 [00:00<00:00, 73.32it/s]\n",
            "Epoch:  47| Step:  20| Train loss: 6.09147: 100%|██████████| 21/21 [00:00<00:00, 72.81it/s]\n",
            "Epoch:  48| Step:  20| Train loss: 7.48303: 100%|██████████| 21/21 [00:00<00:00, 75.64it/s]\n",
            "Epoch:  49| Step:  20| Train loss: 5.86663: 100%|██████████| 21/21 [00:00<00:00, 75.60it/s]\n",
            "Epoch:  50| Step:  20| Train loss: 5.92384: 100%|██████████| 21/21 [00:00<00:00, 75.60it/s]\n",
            "Epoch:  51| Step:  20| Train loss: 6.70435: 100%|██████████| 21/21 [00:00<00:00, 70.84it/s]\n",
            "Epoch:  52| Step:  20| Train loss: 6.90562: 100%|██████████| 21/21 [00:00<00:00, 75.67it/s]\n",
            "Epoch:  53| Step:  20| Train loss: 6.12977: 100%|██████████| 21/21 [00:00<00:00, 67.85it/s]\n",
            "Epoch:  54| Step:  20| Train loss: 5.42050: 100%|██████████| 21/21 [00:00<00:00, 74.16it/s]\n",
            "Epoch:  55| Step:  20| Train loss: 6.83746: 100%|██████████| 21/21 [00:00<00:00, 71.87it/s]\n",
            "Epoch:  56| Step:  20| Train loss: 5.60323: 100%|██████████| 21/21 [00:00<00:00, 74.81it/s]\n",
            "Epoch:  57| Step:  20| Train loss: 8.01175: 100%|██████████| 21/21 [00:00<00:00, 69.30it/s]\n",
            "Epoch:  58| Step:  20| Train loss: 4.89039: 100%|██████████| 21/21 [00:00<00:00, 71.90it/s]\n",
            "Epoch:  59| Step:  20| Train loss: 4.98269: 100%|██████████| 21/21 [00:00<00:00, 73.35it/s]\n",
            "Epoch:  60| Step:  20| Train loss: 4.40137: 100%|██████████| 21/21 [00:00<00:00, 70.66it/s]\n",
            "Epoch:  61| Step:  20| Train loss: 7.20502: 100%|██████████| 21/21 [00:00<00:00, 70.65it/s]\n",
            "Epoch:  62| Step:  20| Train loss: 7.17156: 100%|██████████| 21/21 [00:00<00:00, 67.30it/s]\n",
            "Epoch:  63| Step:  20| Train loss: 5.21546: 100%|██████████| 21/21 [00:00<00:00, 66.66it/s]\n",
            "Epoch:  64| Step:  20| Train loss: 4.36287: 100%|██████████| 21/21 [00:00<00:00, 71.11it/s]\n",
            "Epoch:  65| Step:  20| Train loss: 6.05456: 100%|██████████| 21/21 [00:00<00:00, 66.80it/s]\n",
            "Epoch:  66| Step:  20| Train loss: 5.07840: 100%|██████████| 21/21 [00:00<00:00, 68.24it/s]\n",
            "Epoch:  67| Step:  20| Train loss: 7.17967: 100%|██████████| 21/21 [00:00<00:00, 67.72it/s]\n",
            "Epoch:  68| Step:  20| Train loss: 7.09751: 100%|██████████| 21/21 [00:00<00:00, 64.32it/s]\n",
            "Epoch:  69| Step:  20| Train loss: 5.83512: 100%|██████████| 21/21 [00:00<00:00, 67.46it/s]\n",
            "Epoch:  70| Step:  20| Train loss: 4.34997: 100%|██████████| 21/21 [00:00<00:00, 67.67it/s]\n",
            "Epoch:  71| Step:  20| Train loss: 6.77078: 100%|██████████| 21/21 [00:00<00:00, 68.56it/s]\n",
            "Epoch:  72| Step:  20| Train loss: 5.71440: 100%|██████████| 21/21 [00:00<00:00, 69.49it/s]\n",
            "Epoch:  73| Step:  20| Train loss: 5.64258: 100%|██████████| 21/21 [00:00<00:00, 65.14it/s]\n",
            "Epoch:  74| Step:  20| Train loss: 6.38272: 100%|██████████| 21/21 [00:00<00:00, 67.57it/s]\n",
            "Epoch:  75| Step:  20| Train loss: 6.68971: 100%|██████████| 21/21 [00:00<00:00, 56.90it/s]\n",
            "Epoch:  76| Step:  20| Train loss: 6.62987: 100%|██████████| 21/21 [00:00<00:00, 74.43it/s]\n",
            "Epoch:  77| Step:  20| Train loss: 5.71002: 100%|██████████| 21/21 [00:00<00:00, 58.13it/s]\n",
            "Epoch:  78| Step:  20| Train loss: 5.72989: 100%|██████████| 21/21 [00:00<00:00, 66.26it/s]\n",
            "Epoch:  79| Step:  20| Train loss: 6.23748: 100%|██████████| 21/21 [00:00<00:00, 74.92it/s]\n",
            "Epoch:  80| Step:  20| Train loss: 5.93612: 100%|██████████| 21/21 [00:00<00:00, 75.34it/s]\n",
            "Epoch:  81| Step:  20| Train loss: 6.83289: 100%|██████████| 21/21 [00:00<00:00, 70.99it/s]\n",
            "Epoch:  82| Step:  20| Train loss: 5.54624: 100%|██████████| 21/21 [00:00<00:00, 67.55it/s]\n",
            "Epoch:  83| Step:  20| Train loss: 6.86321: 100%|██████████| 21/21 [00:00<00:00, 74.66it/s]\n",
            "Epoch:  84| Step:  20| Train loss: 7.34784: 100%|██████████| 21/21 [00:00<00:00, 71.97it/s]\n",
            "Epoch:  85| Step:  20| Train loss: 6.36359: 100%|██████████| 21/21 [00:00<00:00, 70.31it/s]\n",
            "Epoch:  86| Step:  20| Train loss: 6.89907: 100%|██████████| 21/21 [00:00<00:00, 73.73it/s]\n",
            "Epoch:  87| Step:  20| Train loss: 6.93652: 100%|██████████| 21/21 [00:00<00:00, 73.99it/s]\n",
            "Epoch:  88| Step:  20| Train loss: 5.19324: 100%|██████████| 21/21 [00:00<00:00, 71.17it/s]\n",
            "Epoch:  89| Step:  20| Train loss: 7.13466: 100%|██████████| 21/21 [00:00<00:00, 69.96it/s]\n",
            "Epoch:  90| Step:  20| Train loss: 6.74353: 100%|██████████| 21/21 [00:00<00:00, 71.90it/s]\n",
            "Epoch:  91| Step:  20| Train loss: 5.93959: 100%|██████████| 21/21 [00:00<00:00, 74.55it/s]\n",
            "Epoch:  92| Step:  20| Train loss: 7.25612: 100%|██████████| 21/21 [00:00<00:00, 72.22it/s]\n",
            "Epoch:  93| Step:  20| Train loss: 7.26820: 100%|██████████| 21/21 [00:00<00:00, 74.24it/s]\n",
            "Epoch:  94| Step:  20| Train loss: 5.57868: 100%|██████████| 21/21 [00:00<00:00, 72.38it/s]\n",
            "Epoch:  95| Step:  20| Train loss: 8.09125: 100%|██████████| 21/21 [00:00<00:00, 70.60it/s]\n",
            "Epoch:  96| Step:  20| Train loss: 7.40326: 100%|██████████| 21/21 [00:00<00:00, 76.00it/s]\n",
            "Epoch:  97| Step:  20| Train loss: 6.69071: 100%|██████████| 21/21 [00:00<00:00, 62.86it/s]\n",
            "Epoch:  98| Step:  20| Train loss: 7.02203: 100%|██████████| 21/21 [00:00<00:00, 58.60it/s]\n",
            "Epoch:  99| Step:  20| Train loss: 6.57635: 100%|██████████| 21/21 [00:00<00:00, 66.20it/s]\n",
            "Epoch: 100| Step:  20| Train loss: 6.01628: 100%|██████████| 21/21 [00:00<00:00, 71.26it/s]\n",
            "Epoch: 101| Step:  20| Train loss: 5.09047: 100%|██████████| 21/21 [00:00<00:00, 73.59it/s]\n",
            "Epoch: 102| Step:  20| Train loss: 4.95982: 100%|██████████| 21/21 [00:00<00:00, 72.58it/s]\n",
            "Epoch: 103| Step:  20| Train loss: 7.07712: 100%|██████████| 21/21 [00:00<00:00, 75.91it/s]\n",
            "Epoch: 104| Step:  20| Train loss: 5.41662: 100%|██████████| 21/21 [00:00<00:00, 76.12it/s]\n",
            "Epoch: 105| Step:  20| Train loss: 5.14734: 100%|██████████| 21/21 [00:00<00:00, 73.95it/s]\n",
            "Epoch: 106| Step:  20| Train loss: 5.27018: 100%|██████████| 21/21 [00:00<00:00, 71.69it/s]\n",
            "Epoch: 107| Step:  20| Train loss: 6.12299: 100%|██████████| 21/21 [00:00<00:00, 74.79it/s]\n",
            "Epoch: 108| Step:  20| Train loss: 6.06625: 100%|██████████| 21/21 [00:00<00:00, 75.84it/s]\n",
            "Epoch: 109| Step:  20| Train loss: 4.81546: 100%|██████████| 21/21 [00:00<00:00, 74.58it/s]\n",
            "Epoch: 110| Step:  20| Train loss: 4.06632: 100%|██████████| 21/21 [00:00<00:00, 75.81it/s]\n",
            "Epoch: 111| Step:  20| Train loss: 7.23725: 100%|██████████| 21/21 [00:00<00:00, 74.43it/s]\n",
            "Epoch: 112| Step:  20| Train loss: 4.59214: 100%|██████████| 21/21 [00:00<00:00, 75.92it/s]\n",
            "Epoch: 113| Step:  20| Train loss: 5.33583: 100%|██████████| 21/21 [00:00<00:00, 68.78it/s]\n",
            "Epoch: 114| Step:  20| Train loss: 7.57954: 100%|██████████| 21/21 [00:00<00:00, 76.12it/s]\n",
            "Epoch: 115| Step:  20| Train loss: 7.67339: 100%|██████████| 21/21 [00:00<00:00, 74.92it/s]\n",
            "Epoch: 116| Step:  20| Train loss: 6.40572: 100%|██████████| 21/21 [00:00<00:00, 67.07it/s]\n",
            "Epoch: 117| Step:  20| Train loss: 4.56856: 100%|██████████| 21/21 [00:00<00:00, 74.37it/s]\n",
            "Epoch: 118| Step:  20| Train loss: 6.50646: 100%|██████████| 21/21 [00:00<00:00, 74.77it/s]\n",
            "Epoch: 119| Step:  20| Train loss: 6.70949: 100%|██████████| 21/21 [00:00<00:00, 74.90it/s]\n",
            "Epoch: 120| Step:  20| Train loss: 7.19009: 100%|██████████| 21/21 [00:00<00:00, 68.23it/s]\n",
            "Epoch: 121| Step:  20| Train loss: 5.01623: 100%|██████████| 21/21 [00:00<00:00, 73.85it/s]\n",
            "Epoch: 122| Step:  20| Train loss: 6.08354: 100%|██████████| 21/21 [00:00<00:00, 74.59it/s]\n",
            "Epoch: 123| Step:  20| Train loss: 7.79722: 100%|██████████| 21/21 [00:00<00:00, 74.96it/s]\n",
            "Epoch: 124| Step:  20| Train loss: 5.95454: 100%|██████████| 21/21 [00:00<00:00, 75.02it/s]\n",
            "Epoch: 125| Step:  20| Train loss: 4.34673: 100%|██████████| 21/21 [00:00<00:00, 74.53it/s]\n",
            "Epoch: 126| Step:  20| Train loss: 4.79584: 100%|██████████| 21/21 [00:00<00:00, 70.59it/s]\n",
            "Epoch: 127| Step:  20| Train loss: 5.52413: 100%|██████████| 21/21 [00:00<00:00, 61.64it/s]\n",
            "Epoch: 128| Step:  20| Train loss: 4.97553: 100%|██████████| 21/21 [00:00<00:00, 76.31it/s]\n",
            "Epoch: 129| Step:  20| Train loss: 7.25623: 100%|██████████| 21/21 [00:00<00:00, 75.87it/s]\n",
            "Epoch: 130| Step:  20| Train loss: 4.42136: 100%|██████████| 21/21 [00:00<00:00, 66.25it/s]\n",
            "Epoch: 131| Step:  20| Train loss: 4.92844: 100%|██████████| 21/21 [00:00<00:00, 73.34it/s]\n",
            "Epoch: 132| Step:  20| Train loss: 7.51018: 100%|██████████| 21/21 [00:00<00:00, 75.80it/s]\n",
            "Epoch: 133| Step:  20| Train loss: 7.00577: 100%|██████████| 21/21 [00:00<00:00, 75.84it/s]\n",
            "Epoch: 134| Step:  20| Train loss: 7.56644: 100%|██████████| 21/21 [00:00<00:00, 71.02it/s]\n",
            "Epoch: 135| Step:  20| Train loss: 6.47271: 100%|██████████| 21/21 [00:00<00:00, 75.91it/s]\n",
            "Epoch: 136| Step:  20| Train loss: 5.20483: 100%|██████████| 21/21 [00:00<00:00, 76.21it/s]\n",
            "Epoch: 137| Step:  20| Train loss: 5.63962: 100%|██████████| 21/21 [00:00<00:00, 76.34it/s]\n",
            "Epoch: 138| Step:  20| Train loss: 5.04729: 100%|██████████| 21/21 [00:00<00:00, 71.64it/s]\n",
            "Epoch: 139| Step:  20| Train loss: 4.84115: 100%|██████████| 21/21 [00:00<00:00, 76.13it/s]\n",
            "Epoch: 140| Step:  20| Train loss: 7.47721: 100%|██████████| 21/21 [00:00<00:00, 75.84it/s]\n",
            "Epoch: 141| Step:  20| Train loss: 6.86077: 100%|██████████| 21/21 [00:00<00:00, 73.63it/s]\n",
            "Epoch: 142| Step:  20| Train loss: 6.42890: 100%|██████████| 21/21 [00:00<00:00, 76.19it/s]\n",
            "Epoch: 143| Step:  20| Train loss: 6.47194: 100%|██████████| 21/21 [00:00<00:00, 75.79it/s]\n",
            "Epoch: 144| Step:  20| Train loss: 7.02295: 100%|██████████| 21/21 [00:00<00:00, 74.40it/s]\n",
            "Epoch: 145| Step:  20| Train loss: 4.95989: 100%|██████████| 21/21 [00:00<00:00, 65.19it/s]\n",
            "Epoch: 146| Step:  20| Train loss: 6.67311: 100%|██████████| 21/21 [00:00<00:00, 75.48it/s]\n",
            "Epoch: 147| Step:  20| Train loss: 3.89830: 100%|██████████| 21/21 [00:00<00:00, 75.88it/s]\n",
            "Epoch: 148| Step:  20| Train loss: 6.56871: 100%|██████████| 21/21 [00:00<00:00, 76.06it/s]\n",
            "Epoch: 149| Step:  20| Train loss: 4.20234: 100%|██████████| 21/21 [00:00<00:00, 73.41it/s]\n",
            "Epoch: 150| Step:  20| Train loss: 6.55612: 100%|██████████| 21/21 [00:00<00:00, 76.11it/s]\n",
            "Epoch: 151| Step:  20| Train loss: 7.11979: 100%|██████████| 21/21 [00:00<00:00, 76.25it/s]\n",
            "Epoch: 152| Step:  20| Train loss: 5.87189: 100%|██████████| 21/21 [00:00<00:00, 72.73it/s]\n",
            "Epoch: 153| Step:  20| Train loss: 5.47497: 100%|██████████| 21/21 [00:00<00:00, 75.70it/s]\n",
            "Epoch: 154| Step:  20| Train loss: 6.85391: 100%|██████████| 21/21 [00:00<00:00, 75.71it/s]\n",
            "Epoch: 155| Step:  20| Train loss: 4.94130: 100%|██████████| 21/21 [00:00<00:00, 76.10it/s]\n",
            "Epoch: 156| Step:  20| Train loss: 6.72610: 100%|██████████| 21/21 [00:00<00:00, 74.46it/s]\n",
            "Epoch: 157| Step:  20| Train loss: 4.00883: 100%|██████████| 21/21 [00:00<00:00, 75.99it/s]\n",
            "Epoch: 158| Step:  20| Train loss: 6.11617: 100%|██████████| 21/21 [00:00<00:00, 76.39it/s]\n",
            "Epoch: 159| Step:  20| Train loss: 4.90400: 100%|██████████| 21/21 [00:00<00:00, 75.54it/s]\n",
            "Epoch: 160| Step:  20| Train loss: 5.62742: 100%|██████████| 21/21 [00:00<00:00, 75.02it/s]\n",
            "Epoch: 161| Step:  20| Train loss: 5.61053: 100%|██████████| 21/21 [00:00<00:00, 75.41it/s]\n",
            "Epoch: 162| Step:  20| Train loss: 6.37573: 100%|██████████| 21/21 [00:00<00:00, 76.05it/s]\n",
            "Epoch: 163| Step:  20| Train loss: 7.08229: 100%|██████████| 21/21 [00:00<00:00, 69.98it/s]\n",
            "Epoch: 164| Step:  20| Train loss: 6.45103: 100%|██████████| 21/21 [00:00<00:00, 75.91it/s]\n",
            "Epoch: 165| Step:  20| Train loss: 7.42035: 100%|██████████| 21/21 [00:00<00:00, 75.86it/s]\n",
            "Epoch: 166| Step:  20| Train loss: 6.43135: 100%|██████████| 21/21 [00:00<00:00, 76.35it/s]\n",
            "Epoch: 167| Step:  20| Train loss: 6.35260: 100%|██████████| 21/21 [00:00<00:00, 71.89it/s]\n",
            "Epoch: 168| Step:  20| Train loss: 6.52447: 100%|██████████| 21/21 [00:00<00:00, 74.24it/s]\n",
            "Epoch: 169| Step:  20| Train loss: 5.55851: 100%|██████████| 21/21 [00:00<00:00, 74.45it/s]\n",
            "Epoch: 170| Step:  20| Train loss: 4.21821: 100%|██████████| 21/21 [00:00<00:00, 67.96it/s]\n",
            "Epoch: 171| Step:  20| Train loss: 5.63082: 100%|██████████| 21/21 [00:00<00:00, 70.28it/s]\n",
            "Epoch: 172| Step:  20| Train loss: 7.35870: 100%|██████████| 21/21 [00:00<00:00, 69.37it/s]\n",
            "Epoch: 173| Step:  20| Train loss: 5.90733: 100%|██████████| 21/21 [00:00<00:00, 66.98it/s]\n",
            "Epoch: 174| Step:  20| Train loss: 5.73508: 100%|██████████| 21/21 [00:00<00:00, 61.31it/s]\n",
            "Epoch: 175| Step:  20| Train loss: 6.79200: 100%|██████████| 21/21 [00:00<00:00, 69.92it/s]\n",
            "Epoch: 176| Step:  20| Train loss: 6.97437: 100%|██████████| 21/21 [00:00<00:00, 74.73it/s]\n",
            "Epoch: 177| Step:  20| Train loss: 5.99274: 100%|██████████| 21/21 [00:00<00:00, 73.19it/s]\n",
            "Epoch: 178| Step:  20| Train loss: 6.06762: 100%|██████████| 21/21 [00:00<00:00, 75.69it/s]\n",
            "Epoch: 179| Step:  20| Train loss: 6.03486: 100%|██████████| 21/21 [00:00<00:00, 75.59it/s]\n",
            "Epoch: 180| Step:  20| Train loss: 7.74077: 100%|██████████| 21/21 [00:00<00:00, 75.89it/s]\n",
            "Epoch: 181| Step:  20| Train loss: 6.11430: 100%|██████████| 21/21 [00:00<00:00, 74.27it/s]\n",
            "Epoch: 182| Step:  20| Train loss: 5.41151: 100%|██████████| 21/21 [00:00<00:00, 75.37it/s]\n",
            "Epoch: 183| Step:  20| Train loss: 5.88154: 100%|██████████| 21/21 [00:00<00:00, 75.72it/s]\n",
            "Epoch: 184| Step:  20| Train loss: 7.09714: 100%|██████████| 21/21 [00:00<00:00, 70.01it/s]\n",
            "Epoch: 185| Step:  20| Train loss: 6.89011: 100%|██████████| 21/21 [00:00<00:00, 75.85it/s]\n",
            "Epoch: 186| Step:  20| Train loss: 6.71564: 100%|██████████| 21/21 [00:00<00:00, 74.85it/s]\n",
            "Epoch: 187| Step:  20| Train loss: 5.66881: 100%|██████████| 21/21 [00:00<00:00, 68.82it/s]\n",
            "Epoch: 188| Step:  20| Train loss: 5.20450: 100%|██████████| 21/21 [00:00<00:00, 60.70it/s]\n",
            "Epoch: 189| Step:  20| Train loss: 5.86388: 100%|██████████| 21/21 [00:00<00:00, 65.62it/s]\n",
            "Epoch: 190| Step:  20| Train loss: 5.60058: 100%|██████████| 21/21 [00:00<00:00, 65.25it/s]\n",
            "Epoch: 191| Step:  20| Train loss: 7.13347: 100%|██████████| 21/21 [00:00<00:00, 68.06it/s]\n",
            "Epoch: 192| Step:  20| Train loss: 7.23707: 100%|██████████| 21/21 [00:00<00:00, 74.18it/s]\n",
            "Epoch: 193| Step:  20| Train loss: 6.17327: 100%|██████████| 21/21 [00:00<00:00, 74.07it/s]\n",
            "Epoch: 194| Step:  20| Train loss: 5.16235: 100%|██████████| 21/21 [00:00<00:00, 73.39it/s]\n",
            "Epoch: 195| Step:  20| Train loss: 5.13625: 100%|██████████| 21/21 [00:00<00:00, 70.99it/s]\n",
            "Epoch: 196| Step:  20| Train loss: 3.05683: 100%|██████████| 21/21 [00:00<00:00, 73.41it/s]\n",
            "Epoch: 197| Step:  20| Train loss: 6.01652: 100%|██████████| 21/21 [00:00<00:00, 73.61it/s]\n",
            "Epoch: 198| Step:  20| Train loss: 6.73004: 100%|██████████| 21/21 [00:00<00:00, 66.98it/s]\n",
            "Epoch: 199| Step:  20| Train loss: 7.12051: 100%|██████████| 21/21 [00:00<00:00, 73.90it/s]\n",
            "Epoch: 200| Step:  20| Train loss: 8.05401: 100%|██████████| 21/21 [00:00<00:00, 55.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NDCG@10: 0.5109266653023485| HIT@10: 0.591\n"
          ]
        }
      ],
      "source": [
        "with mlflow.start_run() as run:\n",
        "    # mlflow.log_param(key=\"max_len\", value=max_len)\n",
        "    # mlflow.log_param(key=\"hidden_units\", value=50)\n",
        "    # mlflow.log_param(key=\"num_heads\", value=1)\n",
        "    # mlflow.log_param(key=\"num_layers\", value=2)\n",
        "    # mlflow.log_param(key=\"dropout_rate\", value=0.5)\n",
        "    # mlflow.log_param(key=\"num_workers\", value=1)\n",
        "    # mlflow.log_param(key=\"device\", value='cuda')\n",
        "    # mlflow.log_param(key=\"lr\", value=0.001)\n",
        "    # mlflow.log_param(key=\"batch_size\", value=128)\n",
        "    # mlflow.log_param(key=\"num_epochs\", value=200)\n",
        "    # mlflow.log_param(key=\"mask_prob\", value=0.15)\n",
        "    \n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        tbar = tqdm(data_loader)\n",
        "        for step, (log_seqs, labels) in enumerate(tbar):\n",
        "            model.train()\n",
        "            logits = model(log_seqs)\n",
        "\n",
        "            # size matching\n",
        "            logits = logits.view(-1, logits.size(-1))\n",
        "            labels = labels.view(-1).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tbar.set_description(f'Epoch: {epoch:3d}| Step: {step:3d}| Train loss: {loss:.5f}')\n",
        "            mlflow.log_metric('loss', loss)\n",
        "    evaluation()\n",
        "    \n",
        "    \n",
        "    # from mlflow.types import Schema, TensorSpec\n",
        "    \n",
        "    # input_schema = Schema([TensorSpec(np.dtype(np.int32), (-1, 20))])\n",
        "    # output_schema = Schema([TensorSpec(np.dtype(np.int32), (20, 8588))])\n",
        "    # signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
        "    \n",
        "    # mlflow.pytorch.log_model(model, \"bert4rec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4j85NeXluiO"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NDCG@10: 0.5256629085257893| HIT@10: 0.61\n"
          ]
        }
      ],
      "source": [
        "evaluation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model, \"/home/ksh/GiftHub_model/modeling/amazon_cf/model/model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BERT4Rec(\n",
              "  (item_emb): Embedding(8589, 50, padding_idx=0)\n",
              "  (pos_emb): Embedding(20, 50)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (emb_layernorm): LayerNorm((50,), eps=1e-06, elementwise_affine=True)\n",
              "  (blocks): ModuleList(\n",
              "    (0): BERT4RecBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_Q): Linear(in_features=50, out_features=50, bias=False)\n",
              "        (W_K): Linear(in_features=50, out_features=50, bias=False)\n",
              "        (W_V): Linear(in_features=50, out_features=50, bias=False)\n",
              "        (W_O): Linear(in_features=50, out_features=50, bias=False)\n",
              "        (attention): ScaledDotProductAttention(\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "        (layerNorm): LayerNorm((50,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "      (pointwise_feedforward): PositionwiseFeedForward(\n",
              "        (W_1): Linear(in_features=50, out_features=200, bias=True)\n",
              "        (W_2): Linear(in_features=200, out_features=50, bias=True)\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "        (layerNorm): LayerNorm((50,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BERT4RecBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_Q): Linear(in_features=50, out_features=50, bias=False)\n",
              "        (W_K): Linear(in_features=50, out_features=50, bias=False)\n",
              "        (W_V): Linear(in_features=50, out_features=50, bias=False)\n",
              "        (W_O): Linear(in_features=50, out_features=50, bias=False)\n",
              "        (attention): ScaledDotProductAttention(\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "        (layerNorm): LayerNorm((50,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "      (pointwise_feedforward): PositionwiseFeedForward(\n",
              "        (W_1): Linear(in_features=50, out_features=200, bias=True)\n",
              "        (W_2): Linear(in_features=200, out_features=50, bias=True)\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "        (layerNorm): LayerNorm((50,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (out): Linear(in_features=50, out_features=8588, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model_1 = BERT4Rec(**params)\n",
        "# model_1.load_state_dict(torch.load(\"/home/ksh/GiftHub_model/modeling/amazon_cf/model/model.pt\"))\n",
        "model_1 = torch.load(\"/home/ksh/GiftHub_model/modeling/amazon_cf/model/model.pt\")\n",
        "model_1.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 인덱스로 변경된 Raw Data 세팅 (user_df)\n",
        "users_dic = defaultdict(list)\n",
        "user_df = {}\n",
        "for u, i, t in zip(df['user_idx'], df['item_idx'], df['time']):\n",
        "    users_dic[u].append(i)\n",
        "    \n",
        "for user in users_dic:\n",
        "    user_df[user] = users_dic[user]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_df[0]\n",
        "# used_items_list = [a - 1 for a in user_df[0]]\n",
        "# used_items_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2578/2578 [00:07<00:00, 331.99it/s]\n"
          ]
        }
      ],
      "source": [
        "# inference\n",
        "model.eval()\n",
        "predict_list = []\n",
        "for u in tqdm(range(num_user)):\n",
        "    seq = (user_df[u])[-max_len:]\n",
        "    used_items_list = [a - 1 for a in user_df[u]]  # 사용한 아이템에 대해 인덱스 계산을 위해 1씩 뺀다.\n",
        "    \n",
        "    if len(seq) < max_len:\n",
        "        seq = np.pad(seq, (max_len - len(seq), 0), 'constant', constant_values=0)  # 패딩 추가\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = -model(np.array([seq]))\n",
        "        predictions = predictions[0][-1][1:]  # mask 제외\n",
        "        predictions[used_items_list] = np.inf  # 사용한 아이템은 제외하기 위해 inf\n",
        "        rank = predictions.argsort().argsort().tolist()\n",
        "        \n",
        "        for i in range(10):\n",
        "            rank.index(i)\n",
        "            predict_list.append([u, rank.index(i)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0, 8580, 8581,\n",
              "       6237, 8582, 4503, 8583, 8584, 8585, 8586, 8587, 8588])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([20, 8588])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(np.array([np.array([0,0,0,0,0,0,0,0,0,5,1,10,15,67,96,84,635,8096,211,182])]))[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0, 60],\n",
              " [0, 1748],\n",
              " [0, 6702],\n",
              " [0, 2395],\n",
              " [0, 3787],\n",
              " [0, 5554],\n",
              " [0, 5167],\n",
              " [0, 7019],\n",
              " [0, 8546],\n",
              " [0, 4832],\n",
              " [1, 1748],\n",
              " [1, 7019],\n",
              " [1, 2395],\n",
              " [1, 2792],\n",
              " [1, 638],\n",
              " [1, 113],\n",
              " [1, 8546],\n",
              " [1, 5949],\n",
              " [1, 7994],\n",
              " [1, 482],\n",
              " [2, 6550],\n",
              " [2, 283],\n",
              " [2, 699],\n",
              " [2, 1748],\n",
              " [2, 638],\n",
              " [2, 6927],\n",
              " [2, 2792],\n",
              " [2, 6702],\n",
              " [2, 1990],\n",
              " [2, 5008],\n",
              " [3, 1098],\n",
              " [3, 4114],\n",
              " [3, 739],\n",
              " [3, 360],\n",
              " [3, 699],\n",
              " [3, 306],\n",
              " [3, 697],\n",
              " [3, 302],\n",
              " [3, 1424],\n",
              " [3, 163],\n",
              " [4, 7019],\n",
              " [4, 1748],\n",
              " [4, 2395],\n",
              " [4, 2792],\n",
              " [4, 4832],\n",
              " [4, 113],\n",
              " [4, 5949],\n",
              " [4, 638],\n",
              " [4, 7866],\n",
              " [4, 5167],\n",
              " [5, 1748],\n",
              " [5, 638],\n",
              " [5, 2395],\n",
              " [5, 7019],\n",
              " [5, 2792],\n",
              " [5, 6702],\n",
              " [5, 3398],\n",
              " [5, 8546],\n",
              " [5, 5448],\n",
              " [5, 5008],\n",
              " [6, 1748],\n",
              " [6, 638],\n",
              " [6, 2395],\n",
              " [6, 2792],\n",
              " [6, 7019],\n",
              " [6, 113],\n",
              " [6, 4703],\n",
              " [6, 5167],\n",
              " [6, 7994],\n",
              " [6, 6702],\n",
              " [7, 6702],\n",
              " [7, 60],\n",
              " [7, 5554],\n",
              " [7, 4832],\n",
              " [7, 6880],\n",
              " [7, 8069],\n",
              " [7, 6748],\n",
              " [7, 1748],\n",
              " [7, 8546],\n",
              " [7, 2110],\n",
              " [8, 1748],\n",
              " [8, 638],\n",
              " [8, 2792],\n",
              " [8, 2395],\n",
              " [8, 5008],\n",
              " [8, 4703],\n",
              " [8, 7994],\n",
              " [8, 4821],\n",
              " [8, 6702],\n",
              " [8, 6550],\n",
              " [9, 5529],\n",
              " [9, 5235],\n",
              " [9, 6820],\n",
              " [9, 7938],\n",
              " [9, 2787],\n",
              " [9, 4109],\n",
              " [9, 908],\n",
              " [9, 6004],\n",
              " [9, 147],\n",
              " [9, 4775],\n",
              " [10, 5529],\n",
              " [10, 5235],\n",
              " [10, 6820],\n",
              " [10, 7938],\n",
              " [10, 2787],\n",
              " [10, 4109],\n",
              " [10, 908],\n",
              " [10, 6004],\n",
              " [10, 147],\n",
              " [10, 4775],\n",
              " [11, 5529],\n",
              " [11, 5235],\n",
              " [11, 6820],\n",
              " [11, 7938],\n",
              " [11, 2787],\n",
              " [11, 4109],\n",
              " [11, 908],\n",
              " [11, 6004],\n",
              " [11, 147],\n",
              " [11, 4775],\n",
              " [12, 638],\n",
              " [12, 1748],\n",
              " [12, 2792],\n",
              " [12, 2395],\n",
              " [12, 7994],\n",
              " [12, 6550],\n",
              " [12, 6927],\n",
              " [12, 4821],\n",
              " [12, 7019],\n",
              " [12, 4703],\n",
              " [13, 5529],\n",
              " [13, 5235],\n",
              " [13, 6820],\n",
              " [13, 7938],\n",
              " [13, 2787],\n",
              " [13, 4109],\n",
              " [13, 908],\n",
              " [13, 6004],\n",
              " [13, 147],\n",
              " [13, 4775],\n",
              " [14, 60],\n",
              " [14, 1748],\n",
              " [14, 5554],\n",
              " [14, 6702],\n",
              " [14, 7019],\n",
              " [14, 2395],\n",
              " [14, 5167],\n",
              " [14, 8546],\n",
              " [14, 3787],\n",
              " [14, 4832],\n",
              " [15, 1748],\n",
              " [15, 638],\n",
              " [15, 6702],\n",
              " [15, 2792],\n",
              " [15, 2395],\n",
              " [15, 6550],\n",
              " [15, 4703],\n",
              " [15, 6927],\n",
              " [15, 4821],\n",
              " [15, 283],\n",
              " [16, 1748],\n",
              " [16, 2792],\n",
              " [16, 638],\n",
              " [16, 7019],\n",
              " [16, 2395],\n",
              " [16, 7994],\n",
              " [16, 482],\n",
              " [16, 1849],\n",
              " [16, 744],\n",
              " [16, 3388],\n",
              " [17, 1748],\n",
              " [17, 638],\n",
              " [17, 2792],\n",
              " [17, 2395],\n",
              " [17, 7019],\n",
              " [17, 6702],\n",
              " [17, 5008],\n",
              " [17, 4703],\n",
              " [17, 113],\n",
              " [17, 4821],\n",
              " [18, 1748],\n",
              " [18, 6702],\n",
              " [18, 2395],\n",
              " [18, 7019],\n",
              " [18, 5554],\n",
              " [18, 3787],\n",
              " [18, 5167],\n",
              " [18, 8546],\n",
              " [18, 6880],\n",
              " [18, 2792],\n",
              " [19, 60],\n",
              " [19, 6702],\n",
              " [19, 5554],\n",
              " [19, 1748],\n",
              " [19, 4832],\n",
              " [19, 535],\n",
              " [19, 2395],\n",
              " [19, 6880],\n",
              " [19, 5167],\n",
              " [19, 7019],\n",
              " [20, 6702],\n",
              " [20, 4832],\n",
              " [20, 5554],\n",
              " [20, 8069],\n",
              " [20, 6748],\n",
              " [20, 6880],\n",
              " [20, 1748],\n",
              " [20, 5167],\n",
              " [20, 5424],\n",
              " [20, 8546],\n",
              " [21, 4832],\n",
              " [21, 1748],\n",
              " [21, 6702],\n",
              " [21, 2395],\n",
              " [21, 7019],\n",
              " [21, 5554],\n",
              " [21, 6748],\n",
              " [21, 8546],\n",
              " [21, 6880],\n",
              " [21, 5167],\n",
              " [22, 1748],\n",
              " [22, 6702],\n",
              " [22, 2395],\n",
              " [22, 7019],\n",
              " [22, 3787],\n",
              " [22, 8546],\n",
              " [22, 4832],\n",
              " [22, 5167],\n",
              " [22, 2792],\n",
              " [22, 5554],\n",
              " [23, 5529],\n",
              " [23, 5235],\n",
              " [23, 6820],\n",
              " [23, 7938],\n",
              " [23, 2787],\n",
              " [23, 4109],\n",
              " [23, 908],\n",
              " [23, 6004],\n",
              " [23, 147],\n",
              " [23, 4775],\n",
              " [24, 4832],\n",
              " [24, 5554],\n",
              " [24, 6702],\n",
              " [24, 2395],\n",
              " [24, 6748],\n",
              " [24, 6880],\n",
              " [24, 1748],\n",
              " [24, 8069],\n",
              " [24, 8546],\n",
              " [24, 7019],\n",
              " [25, 6702],\n",
              " [25, 4832],\n",
              " [25, 5554],\n",
              " [25, 8069],\n",
              " [25, 6880],\n",
              " [25, 6748],\n",
              " [25, 1748],\n",
              " [25, 8546],\n",
              " [25, 2395],\n",
              " [25, 5167],\n",
              " [26, 5529],\n",
              " [26, 5235],\n",
              " [26, 6820],\n",
              " [26, 7938],\n",
              " [26, 2787],\n",
              " [26, 4109],\n",
              " [26, 908],\n",
              " [26, 6004],\n",
              " [26, 147],\n",
              " [26, 4775],\n",
              " [27, 1748],\n",
              " [27, 638],\n",
              " [27, 2792],\n",
              " [27, 7994],\n",
              " [27, 2395],\n",
              " [27, 6550],\n",
              " [27, 6927],\n",
              " [27, 1990],\n",
              " [27, 5008],\n",
              " [27, 283],\n",
              " [28, 6702],\n",
              " [28, 4832],\n",
              " [28, 5554],\n",
              " [28, 8069],\n",
              " [28, 6880],\n",
              " [28, 6748],\n",
              " [28, 2949],\n",
              " [28, 7432],\n",
              " [28, 5424],\n",
              " [28, 4747],\n",
              " [29, 1748],\n",
              " [29, 2395],\n",
              " [29, 7019],\n",
              " [29, 2792],\n",
              " [29, 4832],\n",
              " [29, 6702],\n",
              " [29, 113],\n",
              " [29, 638],\n",
              " [29, 5167],\n",
              " [29, 4703],\n",
              " [30, 1748],\n",
              " [30, 7019],\n",
              " [30, 2395],\n",
              " [30, 638],\n",
              " [30, 2792],\n",
              " [30, 113],\n",
              " [30, 5167],\n",
              " [30, 8546],\n",
              " [30, 3787],\n",
              " [30, 6702],\n",
              " [31, 19],\n",
              " [31, 1098],\n",
              " [31, 360],\n",
              " [31, 699],\n",
              " [31, 6550],\n",
              " [31, 283],\n",
              " [31, 483],\n",
              " [31, 2523],\n",
              " [31, 6927],\n",
              " [31, 244],\n",
              " [32, 6702],\n",
              " [32, 5554],\n",
              " [32, 4832],\n",
              " [32, 6880],\n",
              " [32, 8546],\n",
              " [32, 8069],\n",
              " [32, 6748],\n",
              " [32, 2110],\n",
              " [32, 7432],\n",
              " [32, 1748],\n",
              " [33, 1748],\n",
              " [33, 638],\n",
              " [33, 2792],\n",
              " [33, 2395],\n",
              " [33, 7994],\n",
              " [33, 4821],\n",
              " [33, 7019],\n",
              " [33, 6550],\n",
              " [33, 4703],\n",
              " [33, 6927],\n",
              " [34, 60],\n",
              " [34, 1748],\n",
              " [34, 2395],\n",
              " [34, 7019],\n",
              " [34, 5554],\n",
              " [34, 5167],\n",
              " [34, 4832],\n",
              " [34, 535],\n",
              " [34, 3787],\n",
              " [34, 6796],\n",
              " [35, 5529],\n",
              " [35, 5235],\n",
              " [35, 6820],\n",
              " [35, 7938],\n",
              " [35, 2787],\n",
              " [35, 4109],\n",
              " [35, 908],\n",
              " [35, 6004],\n",
              " [35, 147],\n",
              " [35, 4775],\n",
              " [36, 4832],\n",
              " [36, 5554],\n",
              " [36, 6702],\n",
              " [36, 6880],\n",
              " [36, 8546],\n",
              " [36, 8069],\n",
              " [36, 6748],\n",
              " [36, 2949],\n",
              " [36, 5424],\n",
              " [36, 2395],\n",
              " [37, 1748],\n",
              " [37, 7019],\n",
              " [37, 2395],\n",
              " [37, 2792],\n",
              " [37, 638],\n",
              " [37, 113],\n",
              " [37, 6796],\n",
              " [37, 5167],\n",
              " [37, 8546],\n",
              " [37, 5949],\n",
              " [38, 1748],\n",
              " [38, 7019],\n",
              " [38, 2395],\n",
              " [38, 638],\n",
              " [38, 6702],\n",
              " [38, 2792],\n",
              " [38, 5167],\n",
              " [38, 3787],\n",
              " [38, 5008],\n",
              " [38, 113],\n",
              " [39, 5529],\n",
              " [39, 5235],\n",
              " [39, 56],\n",
              " [39, 6820],\n",
              " [39, 55],\n",
              " [39, 7938],\n",
              " [39, 4109],\n",
              " [39, 306],\n",
              " [39, 6004],\n",
              " [39, 2787],\n",
              " [40, 638],\n",
              " [40, 1748],\n",
              " [40, 2792],\n",
              " [40, 7994],\n",
              " [40, 6424],\n",
              " [40, 3314],\n",
              " [40, 6550],\n",
              " [40, 7019],\n",
              " [40, 1990],\n",
              " [40, 6448],\n",
              " [41, 6702],\n",
              " [41, 5554],\n",
              " [41, 4832],\n",
              " [41, 1748],\n",
              " [41, 2395],\n",
              " [41, 5167],\n",
              " [41, 3787],\n",
              " [41, 7019],\n",
              " [41, 8546],\n",
              " [41, 6748],\n",
              " [42, 1098],\n",
              " [42, 739],\n",
              " [42, 360],\n",
              " [42, 4114],\n",
              " [42, 699],\n",
              " [42, 302],\n",
              " [42, 6290],\n",
              " [42, 697],\n",
              " [42, 163],\n",
              " [42, 1138],\n",
              " [43, 5529],\n",
              " [43, 5235],\n",
              " [43, 6820],\n",
              " [43, 7938],\n",
              " [43, 2787],\n",
              " [43, 4109],\n",
              " [43, 908],\n",
              " [43, 6004],\n",
              " [43, 147],\n",
              " [43, 4775],\n",
              " [44, 6702],\n",
              " [44, 1748],\n",
              " [44, 4832],\n",
              " [44, 2395],\n",
              " [44, 7019],\n",
              " [44, 8546],\n",
              " [44, 113],\n",
              " [44, 2792],\n",
              " [44, 5554],\n",
              " [44, 6748],\n",
              " [45, 1748],\n",
              " [45, 2395],\n",
              " [45, 2792],\n",
              " [45, 638],\n",
              " [45, 7019],\n",
              " [45, 113],\n",
              " [45, 7994],\n",
              " [45, 5167],\n",
              " [45, 4703],\n",
              " [45, 8546],\n",
              " [46, 5529],\n",
              " [46, 5235],\n",
              " [46, 6820],\n",
              " [46, 7938],\n",
              " [46, 2787],\n",
              " [46, 4109],\n",
              " [46, 908],\n",
              " [46, 6004],\n",
              " [46, 147],\n",
              " [46, 4775],\n",
              " [47, 4832],\n",
              " [47, 5554],\n",
              " [47, 1748],\n",
              " [47, 2395],\n",
              " [47, 7019],\n",
              " [47, 5167],\n",
              " [47, 6880],\n",
              " [47, 8546],\n",
              " [47, 60],\n",
              " [47, 3787],\n",
              " [48, 1748],\n",
              " [48, 7019],\n",
              " [48, 2395],\n",
              " [48, 638],\n",
              " [48, 2792],\n",
              " [48, 113],\n",
              " [48, 8546],\n",
              " [48, 5167],\n",
              " [48, 4703],\n",
              " [48, 6702],\n",
              " [49, 1748],\n",
              " [49, 7019],\n",
              " [49, 2395],\n",
              " [49, 2792],\n",
              " [49, 638],\n",
              " [49, 113],\n",
              " [49, 8546],\n",
              " [49, 5949],\n",
              " [49, 4703],\n",
              " [49, 5167],\n",
              " [50, 6702],\n",
              " [50, 1748],\n",
              " [50, 4703],\n",
              " [50, 5008],\n",
              " [50, 638],\n",
              " [50, 5448],\n",
              " [50, 2395],\n",
              " [50, 2792],\n",
              " [50, 3787],\n",
              " [50, 6550],\n",
              " [51, 699],\n",
              " [51, 6550],\n",
              " [51, 283],\n",
              " [51, 6927],\n",
              " [51, 360],\n",
              " [51, 638],\n",
              " [51, 1748],\n",
              " [51, 1990],\n",
              " [51, 483],\n",
              " [51, 2792],\n",
              " [52, 1748],\n",
              " [52, 2395],\n",
              " [52, 7019],\n",
              " [52, 8546],\n",
              " [52, 113],\n",
              " [52, 6702],\n",
              " [52, 2792],\n",
              " [52, 638],\n",
              " [52, 6565],\n",
              " [52, 5167],\n",
              " [53, 1748],\n",
              " [53, 638],\n",
              " [53, 2395],\n",
              " [53, 2792],\n",
              " [53, 7019],\n",
              " [53, 113],\n",
              " [53, 6702],\n",
              " [53, 4703],\n",
              " [53, 4821],\n",
              " [53, 7994],\n",
              " [54, 5529],\n",
              " [54, 5235],\n",
              " [54, 6820],\n",
              " [54, 7938],\n",
              " [54, 2787],\n",
              " [54, 4109],\n",
              " [54, 908],\n",
              " [54, 6004],\n",
              " [54, 147],\n",
              " [54, 4775],\n",
              " [55, 5529],\n",
              " [55, 5235],\n",
              " [55, 6820],\n",
              " [55, 55],\n",
              " [55, 7938],\n",
              " [55, 2787],\n",
              " [55, 4109],\n",
              " [55, 908],\n",
              " [55, 6004],\n",
              " [55, 4775],\n",
              " [56, 306],\n",
              " [56, 3398],\n",
              " [56, 1424],\n",
              " [56, 3314],\n",
              " [56, 5762],\n",
              " [56, 86],\n",
              " [56, 6424],\n",
              " [56, 3495],\n",
              " [56, 744],\n",
              " [56, 4460],\n",
              " [57, 1748],\n",
              " [57, 2395],\n",
              " [57, 7019],\n",
              " [57, 2792],\n",
              " [57, 638],\n",
              " [57, 113],\n",
              " [57, 4832],\n",
              " [57, 6702],\n",
              " [57, 5167],\n",
              " [57, 8546],\n",
              " [58, 1748],\n",
              " [58, 2395],\n",
              " [58, 2792],\n",
              " [58, 638],\n",
              " [58, 7019],\n",
              " [58, 113],\n",
              " [58, 7994],\n",
              " [58, 4703],\n",
              " [58, 5167],\n",
              " [58, 1849],\n",
              " [59, 1748],\n",
              " [59, 7019],\n",
              " [59, 2395],\n",
              " [59, 2792],\n",
              " [59, 113],\n",
              " [59, 638],\n",
              " [59, 6702],\n",
              " [59, 4832],\n",
              " [59, 8546],\n",
              " [59, 5167],\n",
              " [60, 699],\n",
              " [60, 6550],\n",
              " [60, 360],\n",
              " [60, 6927],\n",
              " [60, 1110],\n",
              " [60, 6702],\n",
              " [60, 1990],\n",
              " [60, 244],\n",
              " [60, 1098],\n",
              " [60, 4821],\n",
              " [61, 1748],\n",
              " [61, 638],\n",
              " [61, 2792],\n",
              " [61, 2395],\n",
              " [61, 7019],\n",
              " [61, 113],\n",
              " [61, 7994],\n",
              " [61, 482],\n",
              " [61, 1849],\n",
              " [61, 8546],\n",
              " [62, 6702],\n",
              " [62, 60],\n",
              " [62, 1748],\n",
              " [62, 4832],\n",
              " [62, 5554],\n",
              " [62, 2395],\n",
              " [62, 535],\n",
              " [62, 4703],\n",
              " [62, 3787],\n",
              " [62, 5167],\n",
              " [63, 1748],\n",
              " [63, 2395],\n",
              " [63, 7019],\n",
              " [63, 2792],\n",
              " [63, 4832],\n",
              " [63, 638],\n",
              " [63, 113],\n",
              " [63, 535],\n",
              " [63, 6702],\n",
              " [63, 5949],\n",
              " [64, 19],\n",
              " [64, 1098],\n",
              " [64, 699],\n",
              " [64, 360],\n",
              " [64, 6550],\n",
              " [64, 283],\n",
              " [64, 2523],\n",
              " [64, 244],\n",
              " [64, 1110],\n",
              " [64, 483],\n",
              " [65, 5529],\n",
              " [65, 5235],\n",
              " [65, 6820],\n",
              " [65, 7938],\n",
              " [65, 2787],\n",
              " [65, 4109],\n",
              " [65, 908],\n",
              " [65, 6004],\n",
              " [65, 147],\n",
              " [65, 4775],\n",
              " [66, 8186],\n",
              " [66, 19],\n",
              " [66, 829],\n",
              " [66, 3400],\n",
              " [66, 6907],\n",
              " [66, 60],\n",
              " [66, 1671],\n",
              " [66, 1670],\n",
              " [66, 2241],\n",
              " [66, 2769],\n",
              " [67, 6702],\n",
              " [67, 5554],\n",
              " [67, 4832],\n",
              " [67, 8546],\n",
              " [67, 6748],\n",
              " [67, 1748],\n",
              " [67, 6880],\n",
              " [67, 5448],\n",
              " [67, 8069],\n",
              " [67, 2395],\n",
              " [68, 5529],\n",
              " [68, 5235],\n",
              " [68, 6820],\n",
              " [68, 7938],\n",
              " [68, 4109],\n",
              " [68, 5554],\n",
              " [68, 6004],\n",
              " [68, 147],\n",
              " [68, 535],\n",
              " [68, 908],\n",
              " [69, 6702],\n",
              " [69, 4832],\n",
              " [69, 1748],\n",
              " [69, 5554],\n",
              " [69, 2395],\n",
              " [69, 8546],\n",
              " [69, 7019],\n",
              " [69, 6880],\n",
              " [69, 6748],\n",
              " [69, 113],\n",
              " [70, 1748],\n",
              " [70, 2792],\n",
              " [70, 638],\n",
              " [70, 7019],\n",
              " [70, 2395],\n",
              " [70, 113],\n",
              " [70, 7994],\n",
              " [70, 4703],\n",
              " [70, 3398],\n",
              " [70, 5008],\n",
              " [71, 1748],\n",
              " [71, 2395],\n",
              " [71, 7019],\n",
              " [71, 4832],\n",
              " [71, 113],\n",
              " [71, 2792],\n",
              " [71, 638],\n",
              " [71, 8546],\n",
              " [71, 5949],\n",
              " [71, 5167],\n",
              " [72, 5529],\n",
              " [72, 5235],\n",
              " [72, 6820],\n",
              " [72, 7938],\n",
              " [72, 2787],\n",
              " [72, 4109],\n",
              " [72, 908],\n",
              " [72, 6004],\n",
              " [72, 147],\n",
              " [72, 4775],\n",
              " [73, 6702],\n",
              " [73, 5554],\n",
              " [73, 1748],\n",
              " [73, 8546],\n",
              " [73, 7019],\n",
              " [73, 2395],\n",
              " [73, 6880],\n",
              " [73, 3787],\n",
              " [73, 113],\n",
              " [73, 4832],\n",
              " [74, 3398],\n",
              " [74, 638],\n",
              " [74, 1748],\n",
              " [74, 3314],\n",
              " [74, 2792],\n",
              " [74, 6424],\n",
              " [74, 7994],\n",
              " [74, 1990],\n",
              " [74, 2523],\n",
              " [74, 7019],\n",
              " [75, 4832],\n",
              " [75, 1748],\n",
              " [75, 5554],\n",
              " [75, 2395],\n",
              " [75, 7019],\n",
              " [75, 113],\n",
              " [75, 5167],\n",
              " [75, 535],\n",
              " [75, 6702],\n",
              " [75, 6880],\n",
              " [76, 5529],\n",
              " [76, 5235],\n",
              " [76, 6820],\n",
              " [76, 7938],\n",
              " [76, 2787],\n",
              " [76, 4109],\n",
              " [76, 908],\n",
              " [76, 6004],\n",
              " [76, 147],\n",
              " [76, 4775],\n",
              " [77, 1748],\n",
              " [77, 7019],\n",
              " [77, 2395],\n",
              " [77, 113],\n",
              " [77, 638],\n",
              " [77, 2792],\n",
              " [77, 5949],\n",
              " [77, 6796],\n",
              " [77, 5167],\n",
              " [77, 4832],\n",
              " [78, 6550],\n",
              " [78, 638],\n",
              " [78, 6927],\n",
              " [78, 1748],\n",
              " [78, 2792],\n",
              " [78, 283],\n",
              " [78, 699],\n",
              " [78, 6702],\n",
              " [78, 4821],\n",
              " [78, 1990],\n",
              " [79, 1748],\n",
              " [79, 2395],\n",
              " [79, 2792],\n",
              " [79, 638],\n",
              " [79, 7019],\n",
              " [79, 6702],\n",
              " [79, 4703],\n",
              " [79, 113],\n",
              " [79, 5167],\n",
              " [79, 5008],\n",
              " [80, 638],\n",
              " [80, 1748],\n",
              " [80, 6550],\n",
              " [80, 2792],\n",
              " [80, 6927],\n",
              " [80, 283],\n",
              " [80, 1990],\n",
              " [80, 4821],\n",
              " [80, 699],\n",
              " [80, 2395],\n",
              " [81, 4832],\n",
              " [81, 5554],\n",
              " [81, 6702],\n",
              " [81, 2395],\n",
              " [81, 6880],\n",
              " [81, 6748],\n",
              " [81, 1748],\n",
              " [81, 8069],\n",
              " [81, 8546],\n",
              " [81, 5167],\n",
              " [82, 1748],\n",
              " [82, 6702],\n",
              " [82, 2395],\n",
              " [82, 7019],\n",
              " [82, 4832],\n",
              " [82, 8546],\n",
              " [82, 2792],\n",
              " [82, 638],\n",
              " [82, 113],\n",
              " [82, 5167],\n",
              " [83, 1748],\n",
              " [83, 2395],\n",
              " [83, 7019],\n",
              " [83, 638],\n",
              " [83, 2792],\n",
              " [83, 113],\n",
              " [83, 7994],\n",
              " [83, 5167],\n",
              " [83, 8546],\n",
              " [83, 5949],\n",
              " [84, 1748],\n",
              " [84, 2395],\n",
              " [84, 7019],\n",
              " [84, 2792],\n",
              " [84, 638],\n",
              " [84, 113],\n",
              " [84, 3398],\n",
              " [84, 8546],\n",
              " [84, 5008],\n",
              " [84, 3787],\n",
              " [85, 1748],\n",
              " [85, 2395],\n",
              " [85, 2792],\n",
              " [85, 7019],\n",
              " [85, 638],\n",
              " [85, 6702],\n",
              " [85, 113],\n",
              " [85, 8546],\n",
              " [85, 3787],\n",
              " [85, 7994],\n",
              " [86, 1748],\n",
              " [86, 2395],\n",
              " [86, 7019],\n",
              " [86, 2792],\n",
              " [86, 638],\n",
              " [86, 113],\n",
              " [86, 5167],\n",
              " [86, 4832],\n",
              " [86, 5949],\n",
              " [86, 8546],\n",
              " [87, 5554],\n",
              " [87, 6702],\n",
              " [87, 4832],\n",
              " [87, 8069],\n",
              " [87, 6880],\n",
              " [87, 8546],\n",
              " [87, 6748],\n",
              " [87, 5424],\n",
              " [87, 2949],\n",
              " [87, 5167],\n",
              " [88, 8186],\n",
              " [88, 829],\n",
              " [88, 19],\n",
              " [88, 3400],\n",
              " [88, 6907],\n",
              " [88, 1671],\n",
              " [88, 2241],\n",
              " [88, 1670],\n",
              " [88, 2769],\n",
              " [88, 5239],\n",
              " [89, 5529],\n",
              " [89, 5235],\n",
              " [89, 6820],\n",
              " [89, 7938],\n",
              " [89, 2787],\n",
              " [89, 4109],\n",
              " [89, 908],\n",
              " [89, 6004],\n",
              " [89, 147],\n",
              " [89, 4775],\n",
              " [90, 5529],\n",
              " [90, 5235],\n",
              " [90, 6820],\n",
              " [90, 7938],\n",
              " [90, 2787],\n",
              " [90, 4109],\n",
              " [90, 908],\n",
              " [90, 6004],\n",
              " [90, 147],\n",
              " [90, 4775],\n",
              " [91, 5529],\n",
              " [91, 5235],\n",
              " [91, 6820],\n",
              " [91, 7938],\n",
              " [91, 2787],\n",
              " [91, 4109],\n",
              " [91, 908],\n",
              " [91, 6004],\n",
              " [91, 147],\n",
              " [91, 4775],\n",
              " [92, 1748],\n",
              " [92, 2395],\n",
              " [92, 638],\n",
              " [92, 2792],\n",
              " [92, 7019],\n",
              " [92, 113],\n",
              " [92, 1849],\n",
              " [92, 5167],\n",
              " [92, 7994],\n",
              " [92, 535],\n",
              " [93, 1748],\n",
              " [93, 7019],\n",
              " [93, 2395],\n",
              " [93, 638],\n",
              " [93, 6796],\n",
              " [93, 2792],\n",
              " [93, 60],\n",
              " [93, 113],\n",
              " [93, 7994],\n",
              " [93, 2663],\n",
              " [94, 6550],\n",
              " [94, 699],\n",
              " [94, 283],\n",
              " [94, 6927],\n",
              " [94, 1990],\n",
              " [94, 638],\n",
              " [94, 4821],\n",
              " [94, 1748],\n",
              " [94, 2792],\n",
              " [94, 1110],\n",
              " [95, 1748],\n",
              " [95, 6702],\n",
              " [95, 2395],\n",
              " [95, 638],\n",
              " [95, 2792],\n",
              " [95, 7019],\n",
              " [95, 4703],\n",
              " [95, 5167],\n",
              " [95, 5008],\n",
              " [95, 8546],\n",
              " [96, 1748],\n",
              " [96, 638],\n",
              " [96, 2792],\n",
              " [96, 6702],\n",
              " [96, 2395],\n",
              " [96, 7019],\n",
              " [96, 4821],\n",
              " [96, 113],\n",
              " [96, 4703],\n",
              " [96, 5448],\n",
              " [97, 6702],\n",
              " [97, 4832],\n",
              " [97, 5554],\n",
              " [97, 1748],\n",
              " [97, 6880],\n",
              " [97, 2395],\n",
              " [97, 6748],\n",
              " [97, 8546],\n",
              " [97, 5167],\n",
              " [97, 8069],\n",
              " [98, 5529],\n",
              " [98, 5235],\n",
              " [98, 6820],\n",
              " [98, 7938],\n",
              " [98, 2787],\n",
              " [98, 4109],\n",
              " [98, 908],\n",
              " [98, 6004],\n",
              " [98, 147],\n",
              " [98, 4775],\n",
              " [99, 1748],\n",
              " [99, 6702],\n",
              " [99, 5554],\n",
              " [99, 2395],\n",
              " [99, 4832],\n",
              " [99, 60],\n",
              " [99, 7019],\n",
              " [99, 8546],\n",
              " [99, 5167],\n",
              " [99, 3787],\n",
              " ...]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Export\n",
        "# 인덱스를 원래 데이터 상태로 변환하여 csv 저장합니다.\n",
        "predict_list_idx = [[user2idx.index[user], item2idx.index[item]] for user, item in predict_list]\n",
        "predict_df = pd.DataFrame(data=predict_list_idx, columns=['user', 'item'])\n",
        "predict_df = predict_df.sort_values('user')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>item</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [user, item]\n",
              "Index: []"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jHiLnX2n83B"
      },
      "source": [
        "## Required Package\n",
        "\n",
        "- numpy==1.23.5\n",
        "- pandas==1.5.3\n",
        "- torch==2.1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R9qHrdCJsMy"
      },
      "source": [
        "###**콘텐츠 라이선스**\n",
        "\n",
        "<font color='red'><b>**WARNING**</b></font> : **본 교육 콘텐츠의 지식재산권은 재단법인 네이버커넥트에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다.** 다만, 비영리적 교육 및 연구활동에 한정되어 사용할 수 있으나 재단의 허락을 받아야 합니다. 이를 위반하는 경우, 관련 법률에 따라 책임을 질 수 있습니다.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
